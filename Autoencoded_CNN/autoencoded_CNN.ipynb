{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\Gamer\\\\OneDrive\\\\Desktop\\\\space_apps_2024_seismic_detection\\\\space_apps_2024_seismic_detection\\\\data\\\\lunar\\\\training\\\\data\\\\S12_GradeA\\\\xa.s12.00.mhz.1970-01-19HR00_evid00002.csv\"\n",
    "file = pd.read_csv(path)\n",
    "df = pd.DataFrame(file)\n",
    "df['velocity(m/s)'] = np.abs(df['velocity(m/s)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[0], axis=1)\n",
    "\n",
    "time_steps = 120 // (df['time_rel(sec)'].iloc[2] - df['time_rel(sec)'].iloc[1])#Assuming the average earthquake is around 2 minutes --May need adjustment.\n",
    "time_steps = time_steps.astype(int)\n",
    "print(f\"time_steps: {time_steps}\")\n",
    "\n",
    "df = df[df['velocity(m/s)'] > 1e-13]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = normalize(df, norm='l2', axis=0)\n",
    "df_norm = pd.DataFrame(norm, columns=df.columns)\n",
    "df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = df_norm.shape[0] // time_steps\n",
    "print(f\"n_samples: {n_samples}, {n_samples.dtype}\")\n",
    "\n",
    "velocity_data = df_norm['velocity(m/s)'].values[:n_samples * time_steps].reshape(n_samples, time_steps, 1)\n",
    "velocity_data = velocity_data.astype(np.float32)\n",
    "print(f\"velocity_data shape: {velocity_data.shape}, {n_samples.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, UpSampling1D, Dense, Flatten, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Autoencoder:\n",
    "    def __init__(self, time_steps, n_filters, kernel_size, pool_size, latent_dim, epochs, batch_size, optimizer='adam', loss='mse'):\n",
    "        self.time_steps = time_steps\n",
    "        self.n_filters = n_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool_size = pool_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.autoencoder = None\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def build_model(self):\n",
    "        input_layer = Input(shape=(self.time_steps, 1))\n",
    "\n",
    "        x = input_layer\n",
    "        for filter in self.n_filters:\n",
    "            x = Conv1D(filter, kernel_size=self.kernel_size, activation='relu', padding='same')(x)\n",
    "            x = MaxPooling1D(pool_size=self.pool_size, padding='same')(x)\n",
    "\n",
    "        latent = Conv1D(self.latent_dim, kernel_size=self.kernel_size, activation='relu', padding='same')(x)\n",
    "\n",
    "        x = latent\n",
    "        for filters in reversed(self.n_filters):\n",
    "            x = Conv1D(filters, kernel_size=self.kernel_size, activation='relu', padding='same')(x)\n",
    "            x = UpSampling1D(size=self.pool_size)(x)\n",
    "\n",
    "        decoded = Conv1D(1, kernel_size=self.kernel_size, activation='sigmoid', padding='same')(x)\n",
    "        decoded = tf.keras.layers.Cropping1D(cropping=(0, 1))(decoded)\n",
    "\n",
    "        self.autoencoder = Model(input_layer, decoded)\n",
    "        self.autoencoder.compile(optimizer=self.optimizer, loss=self.loss)\n",
    "\n",
    "        self.autoencoder.summary()\n",
    "\n",
    "    def train(self, data):\n",
    "        history = self.autoencoder.fit(data, data, epochs=self.epochs, batch_size=self.batch_size)\n",
    "        return history\n",
    "    \n",
    "    def reconstruct(self, data):\n",
    "        return self.autoencoder.predict(data)\n",
    "\n",
    "    def calculate_error(self, data, recon_data):\n",
    "        mse = np.mean(np.power(data - recon_data, 2), axis=1)\n",
    "        return mse\n",
    "    \n",
    "    def detect_anomalies(self, data, threshold_percentile=95):\n",
    "        reconstructed_data = self.reconstruct(data)\n",
    "        reconstruction_error = self.calculate_error(data, reconstructed_data)\n",
    "        threshold = np.percentile(reconstruction_error, threshold_percentile)\n",
    "        anomalies = (reconstruction_error > threshold)\n",
    "        return anomalies, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df\n",
    "df1['Labels'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_autoencoder = CNN_Autoencoder(\n",
    "    time_steps=time_steps,\n",
    "    n_filters=[32, 16],\n",
    "    kernel_size=3,\n",
    "    pool_size=2,\n",
    "    latent_dim=8,\n",
    "    epochs=50,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "cnn_autoencoder.build_model()\n",
    "history = cnn_autoencoder.train(velocity_data)\n",
    "print(history.history)\n",
    "\n",
    "anomalies, threshold = cnn_autoencoder.detect_anomalies(velocity_data, threshold_percentile=95)\n",
    "\n",
    "for i in range(len(anomalies)): \n",
    "    for j in range(i * time_steps, time_steps + (i * time_steps)):\n",
    "        df['Labels'].iloc[j] = anomalies[i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
